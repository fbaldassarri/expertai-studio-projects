Natural-language understanding or natural-language interpretation (NLU and NLI respectively) is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.
There is considerable commercial interest in the field because of its application to automated reasoning, machine translation, question answering, news-gathering, text categorization, voice-activation, archiving, and large-scale content analysis.
The program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at Massachusetts Institute of Technology (Cambridge, Massachusetts) is one of the earliest known attempts by a computer at natural-language analysis and understanding. Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled Natural Language Input for a Computer Problem Solving System) showed how a computer could understand simple natural language input to solve algebra word problems.
A year later, in 1965, Joseph Weizenbaum at MIT wrote ELIZA, an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a database of real-world knowledge or a rich lexicon. Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by Ask.com.
In 1969 Roger Schank at Stanford University (Stanford, California) introduced the conceptual dependency theory for natural-language understanding. This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University (New Haven, Connecticut), such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.
In 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input. Instead of phrase structure rules ATNs used an equivalent set of finite automata that were called recursively. ATNs and their more general format called "generalized ATNs" continued to be used for a number of years.
In 1971 Terry Winograd finished writing SHRDLU for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field. Winograd continued to be a major influence in the field with the publication of his book Language as a Cognitive Process. At Stanford, Winograd would later be the adviser for Larry Page, who co-founded Google.
In the 1970s and 1980s the natural language processing group at SRI International continued research and development in the field. A number of commercial efforts based on the research were undertaken, e.g., in 1982 Gary Hendrix formed Symantec Corporation originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse driven, graphic user interfaces Symantec changed direction. A number of other commercial efforts were started around the same time, e.g., Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems corp. In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnert.
The third millennium saw the introduction of systems using machine learning for text classification, such as the IBM Watson. However, it is debated how much "understanding" such systems demonstrate, e.g. according to John Searle, Watson did not even understand the questions.
John Ball, cognitive scientist and inventor of Patom Theory supports this assessment. Natural language processing has made inroads for applications to support human productivity in service and ecommerce but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language which still defies conventional natural language processing. "To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence - just like a 3-year-old does without guesswork" Patom Theory.
